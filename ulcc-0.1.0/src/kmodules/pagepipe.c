/*
 * Copyright (C) 2011 Xiaoning Ding, Kaibo Wang, Xiaodong Zhang
 *
 * This file is part of ULCC (User Level Cache Control) utility. ULCC is free
 * software: you can redistribute it and/or modify it under the terms of the
 * GNU General Public License as published by the Free Software Foundation.
 * Read the file COPYING for details of GNU GPL.
 */

#include <linux/module.h>
#include <linux/kernel.h>
#include <linux/init.h>
#include <linux/fs.h>
#include <linux/version.h>
#include <linux/debugfs.h>
#include <linux/slab.h>
#include <asm/uaccess.h>
#include <asm/pgtable.h>
#include <asm/highmem.h>
#include <linux/mm.h>  /* mmap related stuff */
#include <linux/spinlock.h>
#include "pagepipe.h"

#define NR_PIPES 64
#define DEBUG

/* To request a number of pages in specific colors, an application first notifies
 * the memory manager, and waits for memory manager ``allocating'' pages. The application
 * need to specify the number of the pages and their colors. The allocated pages are 
 * ``passed'' from memory manager to the application through a ``page pipe''. By ``pass'', 
 * we mean the pages being mapped to the VM space of the application. To grant the memory
 * manager the access to the page pipe, the application also need to provide the memory
 * manager with the ID of the page pipe and a randomly-generated key. 
 *
 * The memory manager is in charge of puting the pages (virtual addresses) into pipe with 
 * appropriate order
 * to guarantee the colors of consecutive pages are evenly distributed. When page faults
 * incur, the module will map the pages in the same order as they are placed in the pipe. 
 *
 * The memory manager ``puts'' the allocated pages into the page pipe by calling write(2) 
 * system call. Then the application does a memory mapping and triggers page faults to 
 * acquire the pages. 
 *
 * To get the number of unused pages, the memory manager calls ioctl(IOCTL_NUM_UNUSED, &num).
 * To get the VM addresses to unused pages, the memory manager calls read(2). 
 * 
 * The page pipe is built when the application calls ioctl(IOCTL_SETUP_PIPE, key) (suppose 
 * the device file has already been opened). The application need to specify the key in the 
 * last argument in ioctl call. When the page pipe is established, the ID is returned.
 */
struct single_page{
	struct list_head page_list;
	struct page *this_page;
	unsigned long vm_address;
};

struct page_pipe {
	spinlock_t lock;		/* the lock protecting the pipe */
	unsigned long nr_pages;		/* number of pages in the pipe */
/* number of pages that have not been reserved */
	unsigned long nr_unreserved_pages; 
	unsigned long pipe_id;
	struct list_head page_list; 	/* lists of pages */
/* a random number generated by app. The module uses this module to confirm 
 * that the memmanager is really passing the pages to a specific application. 
 * When an app asks for some pages from memmanager, it first generates a key. 
 * Then it tells the key to both memmanager and the kernel module. When the 
 * memmanager passes some pages, it needs to provide this key, so that the 
 * module can make sure that the pages are really for the application */
	unsigned long key;	
};

static struct page_pipe pipes[NR_PIPES];

/* An application may use fewer pages than what they have requested. Unused pages are put 
 * on to this list for memory manager to recycle
 */
static spinlock_t unused_pages_lock;
static struct list_head unused_pages; 
static long nr_unused_pages;

#define last_single_page(_head) (list_entry((_head)->prev, struct single_page, page_list))

/* we don't support huge pages */
static unsigned long translate(struct mm_struct *mm, unsigned long address)
{
        pgd_t *pgd;
        pud_t *pud;
        pmd_t *pmd;
        pte_t *ptep, pte;
        unsigned long page_address, pfn;

		page_address = address & PAGE_MASK;
        pgd = pgd_offset(mm, page_address);
        if (pgd_none(*pgd) || unlikely(pgd_bad(*pgd)))
                goto out;

        pud = pud_offset(pgd, page_address);
        if (pud_none(*pud) || unlikely(pud_bad(*pud)))
                goto out;

        pmd = pmd_offset(pud, page_address);
        if (pmd_none(*pmd) || unlikely(pmd_bad(*pmd)))
                goto out;

        ptep = pte_offset_map(pmd, page_address);
        if (!ptep)
                goto out;

        pte = *ptep;
        pte_unmap(ptep);
        if (pte_present(pte)) {
                pfn = pte_pfn(pte);
                if (pfn_valid(pfn)) {
                        return pfn;
                }
        }

out:
	return 0;
}


/* a pages in one of the desire colors is allocated on page fault */
#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,23)
int mmap_fault(struct vm_area_struct *vma, struct vm_fault *vmf)
{
	struct page *pg;
	unsigned long pipe_ID = (unsigned long)vma->vm_private_data;
	struct page_pipe *pipe;
	struct single_page *pg_rec;

	BUG_ON(pipe_ID >= NR_PIPES);
	pipe = pipes + pipe_ID;
	spin_lock(&pipe->lock);
	pg_rec = last_single_page(&pipe->page_list);
	list_del(&pg_rec->page_list);
	pipe->nr_pages--;
	spin_unlock(&pipe->lock);
	pg = pg_rec->this_page;
	kfree(pg_rec);
	
	
/*	vm_insert_page(vma, (unsigned long)vmf->virtual_address, pg);*/
	get_page(pg);
	vmf->page = pg;

/*	unsigned long pfn;
	pfn = page_to_pfn(pg);
*/
	return 0;
}
#else
#endif



/* unsigned long pipe_ID;
 * unsigned long key;
 * n * unsigned long virtual addresses;
 */
static ssize_t
dev_write(struct file *file,
	     const char __user * buffer, size_t length, loff_t * foffset)
{
	unsigned long pipe_ID, key;
	struct page_pipe *pipe;
	struct list_head page_list, *pos;
	unsigned long address, pfn;
	int offset;
	struct page *pg;
	struct single_page *pg_rec;
	unsigned long nr_pages;

#ifdef DEBUG
	printk(KERN_INFO "dev_write %lu\n", length);
#endif
	if( length < 3 * sizeof(unsigned long) )
		return -EINVAL;
	copy_from_user(&pipe_ID, buffer, sizeof(unsigned long));
	copy_from_user(&key, buffer + sizeof(unsigned long), sizeof(unsigned long));
	if(pipe_ID >= NR_PIPES)
		return -EINVAL;

	pipe = pipes + pipe_ID;
	if( key != pipe->key || pipe_ID != pipe->pipe_id )
		return -EINVAL;

	/* put all the pages on to a temporary list */
	nr_pages = 0;
	INIT_LIST_HEAD(&page_list);
	for( offset = 2 * sizeof(unsigned long); offset < length; offset += sizeof(unsigned long) ) {
		copy_from_user(&address, buffer+offset, sizeof(unsigned long));
		pfn = translate(current->mm, address);
		pg = pfn_to_page(pfn);
		if(pg!=NULL) {
			pg_rec = kmalloc(sizeof(struct single_page), GFP_KERNEL);
			pg_rec->this_page = pg;
			pg_rec->vm_address = address;
			list_add(&pg_rec->page_list, &page_list);
			nr_pages++;
		}
	}

	spin_lock(&pipe->lock);
	if( key != pipe->key || pipe_ID != pipe->pipe_id ){
		spin_unlock(&pipe->lock);
		goto broken;
	}
	list_splice(&page_list, &pipe->page_list);
	pipe->nr_pages += nr_pages;
	pipe->nr_unreserved_pages += nr_pages;
	spin_unlock(&pipe->lock);

#ifdef DEBUG
	printk(KERN_INFO "dev_write(%d): pipe->nr_pg: %lu pipe->nr_pg_unres: %lu \n", __LINE__, pipe->nr_pages, pipe->nr_unreserved_pages);
#endif
	return length;

broken:
	/* free all the single_page structs on the temporary list */
	list_for_each(pos, &page_list){
		pg_rec = list_entry(pos, struct single_page, page_list);
		list_del(pos);
		kfree(pg_rec);
	}

	return -EINVAL;
}

/* for client to specify the key and to get the ID of the pipe */
int dev_ioctl(struct inode *inode, struct file *file,
		 unsigned int ioctl_num, unsigned long ioctl_param)
{
	struct page_pipe *pipe;
	unsigned long iter = 0;
	unsigned long key;
	unsigned long pipe_ID;

#ifdef DEBUG
	printk(KERN_INFO "dev_ioctl %u\n", ioctl_num);
#endif
	switch (ioctl_num)
	{
		case IOCTL_SETUP_PIPE:
			goto setup_pipe;
		case IOCTL_NUM_UNUSEDPAGES:
			goto unusedpages;
		default:
			return -EINVAL;
	}

unusedpages:
	return nr_unused_pages;

setup_pipe:   /**********************/
	key = ioctl_param;
	pipe_ID = (unsigned long) file->private_data;
	if( pipe_ID < NR_PIPES )
		return -EINVAL;

	/* find a pipe not in use */
again:
	for( pipe = pipes; pipe < pipes + NR_PIPES; pipe++) {
		if(pipe->pipe_id > NR_PIPES) {
			spin_lock(&pipe->lock);
			if( likely(pipe->pipe_id > NR_PIPES) )
				break;
			spin_unlock(&pipe->lock);
		}
	}

	if( pipe == pipes + NR_PIPES) {
		iter++;
		if(iter > 5)
			return -EAGAIN;
		else
			goto again;
	}

	pipe->nr_pages = 0;
	pipe->nr_unreserved_pages = 0;
	pipe->key = key;
	pipe_ID = pipe - pipes;
	pipe->pipe_id = pipe_ID;
	file->private_data = (void *)pipe_ID;
	spin_unlock(&pipe->lock);

	/* return the pipe ID to the application */
	return pipe_ID;
}

/* put the VM addresses of first length/sizeof(unsigned long) pages
 * into buffer. The module releases the records of these pages. 
 */
static ssize_t dev_read(struct file *file,
	     char __user * buffer, size_t length, loff_t * offset)
{
	struct list_head *pos;
	unsigned long pointer;
	struct single_page *pg_rec;

#ifdef DEBUG
	printk(KERN_INFO "dev_read.\n");
#endif
	pointer = 0;
	spin_lock(&unused_pages_lock);
	list_for_each(pos, &unused_pages){
		pg_rec = list_entry(pos, struct single_page, page_list);
		copy_to_user(buffer + pointer, &pg_rec->vm_address, sizeof(unsigned long));
		pointer += sizeof(unsigned long);
		list_del(pos);
		kfree(pg_rec);
		if(length - pointer < sizeof(unsigned long))
			break;
	}
	spin_unlock(&unused_pages_lock);

	return pointer;
}

/*
 * put unused pages into a common list.
 */
int dev_close(struct inode *inode, struct file *file)
{
	struct page_pipe *pipe;
	unsigned long pipe_ID  = (unsigned long)file->private_data;
	struct list_head page_list;
	unsigned long nr_pages;

#ifdef DEBUG
	printk(KERN_INFO "dev_close.\n");
#endif
	if(pipe_ID >= NR_PIPES) 
		return 0;

	pipe = pipes + pipe_ID;

	INIT_LIST_HEAD(&page_list);

	spin_lock(&pipe->lock);
	/* move unused pages to a temporary list */
	nr_pages = pipe->nr_pages;
	if(nr_pages != 0)
		list_splice(&(pipe->page_list), &page_list);

    	pipe->pipe_id = NR_PIPES + 1;
	pipe->key = 0;
	file->private_data = (void *)(NR_PIPES + 1);
	pipe->nr_pages = 0;
	pipe->nr_unreserved_pages = 0;
	spin_unlock(&pipe->lock);

	/* then move the unused pages to "unused_pages" list */
	if(nr_pages != 0) {
		spin_lock(&unused_pages_lock);
		list_splice(&page_list, &unused_pages);
		nr_unused_pages += nr_pages;
		spin_unlock(&unused_pages_lock);
	}
	return 0;
}

struct vm_operations_struct mmap_vm_ops = {
#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,23)
	.fault = mmap_fault,
#else
	.nopage =   mmap_nopage,
#endif
};

/* admission control here is to avoid an application asking for fewer pages
 * with dev_read but requesting more pages by mmap-ing large areas.
 */
int dev_mmap(struct file *filp, struct vm_area_struct *vma)
{
	unsigned long size, nr_pages;
	unsigned long pipe_ID = (unsigned long)filp->private_data;
	struct page_pipe *pipe = pipes + pipe_ID;

#ifdef DEBUG
	printk(KERN_INFO "dev_mmap\n");
#endif

	size = vma->vm_end - vma->vm_start;	/*JERRY: are vma->vm-start and vma->vm_end always page aligned? */
	if( (size & (PAGE_SIZE - 1)) != 0 )
		return -EINVAL;

	nr_pages = size >> PAGE_SHIFT;

	spin_lock(&pipe->lock);
	if( nr_pages > pipe->nr_unreserved_pages ){
		spin_unlock(&pipe->lock);
		return -EINVAL;
	}
	else 
		pipe->nr_unreserved_pages -= nr_pages;
	spin_unlock(&pipe->lock);

	vma->vm_ops = &mmap_vm_ops;
	/*vma->vm_flags |= VM_RESERVED;*/
	/* assign the file private data to the vm private data */
	vma->vm_private_data = filp->private_data;
	
	return 0;
}

int dev_open(struct inode *inode, struct file *filp)
{
#ifdef DEBUG
	printk(KERN_INFO "dev_open\n");
#endif
	filp->private_data = (void *)(NR_PIPES + 1);
	return 0;
}

static const struct file_operations Fops = {
	.open = dev_open,
	.read = dev_read,
	.mmap = dev_mmap,
	.write = dev_write,
	.release = dev_close,
	.ioctl = dev_ioctl,
};

int init_module()
{
	int i, ret_val;

	ret_val = register_chrdev(MAJOR_NUM, DEVICE_NAME, &Fops);
	if (ret_val < 0) {
		printk(KERN_ALERT "Registering the character device failed with %d.", ret_val);
		return ret_val;
	}

	for( i = 0; i < NR_PIPES; i++) {
		pipes[i].pipe_id = NR_PIPES + 1;
		spin_lock_init(&pipes[i].lock);
		INIT_LIST_HEAD(&(pipes[i].page_list));
	}

	INIT_LIST_HEAD(&unused_pages);
	spin_lock_init(&unused_pages_lock);
	nr_unused_pages = 0;

	printk(KERN_INFO "Registeration is a success. The major device number is %d.\n",  MAJOR_NUM);
	printk(KERN_INFO "If you want to talk to the device driver, you'll have to create a device file. \n");
	printk(KERN_INFO "Suggest: mknod /tmp/%s c %d 0\n", DEVICE_FILE_NAME, MAJOR_NUM);

	return 0;
}

void cleanup_module()
{
	struct page_pipe *pipe;
	struct list_head *pos;
	struct single_page *pg_rec;
	int i;

	for( i = 0; i < NR_PIPES; i++) {
		pipe = pipes + i;
		if(pipe->nr_pages > 0) {
			list_for_each(pos, &(pipe->page_list)){
				pg_rec = list_entry(pos, struct single_page, page_list);
				list_del(pos);
				kfree(pg_rec);
			}
		}
	}

	list_for_each(pos, &(unused_pages)){
		pg_rec = list_entry(pos, struct single_page, page_list);
		list_del(pos);
		kfree(pg_rec);
	}

	unregister_chrdev(MAJOR_NUM, DEVICE_NAME);
	printk(KERN_INFO "Module pagepipe is removed.\n");
}

MODULE_LICENSE("GPL");
